{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shifu/.conda/envs/fid4/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "from src.options import Options\n",
    "\n",
    "import src.slurm\n",
    "import src.util\n",
    "import src.evaluation\n",
    "import src.data\n",
    "import src.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path):\n",
    "\n",
    "    if opt.is_main:\n",
    "        try:\n",
    "            tb_logger = torch.utils.tensorboard.SummaryWriter(Path(opt.checkpoint_dir)/opt.name)\n",
    "        except:\n",
    "            tb_logger = None\n",
    "            logger.warning('Tensorboard is not available.')\n",
    "\n",
    "    torch.manual_seed(opt.global_rank + opt.seed) #different seed for different sampling depending on global_rank\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=opt.per_gpu_batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=10,\n",
    "        collate_fn=collator\n",
    "    )\n",
    "\n",
    "    loss, curr_loss = 0.0, 0.0\n",
    "    epoch = 1\n",
    "    model.train()\n",
    "    while step < opt.total_steps:\n",
    "        epoch += 1\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            step += 1\n",
    "            (idx, labels, _, context_ids, context_mask) = batch\n",
    "\n",
    "            train_loss = model(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                labels=labels.cuda()\n",
    "            )[0]\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            if step % opt.accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "            train_loss = src.util.average_main(train_loss, opt)\n",
    "            curr_loss += train_loss.item()\n",
    "\n",
    "            if step % opt.eval_freq == 0:\n",
    "                dev_em = evaluate(model, eval_dataset, tokenizer, collator, opt ,step)\n",
    "                model.train()\n",
    "                if opt.is_main:\n",
    "                    # if dev_em > best_dev_em:\n",
    "                    #     best_dev_em = dev_em\n",
    "                    #     src.util.save(model, optimizer, scheduler, step, best_dev_em,\n",
    "                    #               opt, checkpoint_path, 'best_dev')\n",
    "                    log = f\"{step} / {opt.total_steps} |\"\n",
    "                    log += f\"train: {curr_loss/opt.eval_freq:.3f} |\"\n",
    "                    log += f\"evaluation: {100:.2f}EM |\"\n",
    "                    log += f\"lr: {scheduler.get_last_lr()[0]:.5f}\"\n",
    "                    logger.info(log)    \n",
    "                    if tb_logger is not None:\n",
    "                        tb_logger.add_scalar(\"Evaluation\", dev_em, step)\n",
    "                        tb_logger.add_scalar(\"Training\", curr_loss / (opt.eval_freq), step)\n",
    "                    curr_loss = 0.\n",
    "\n",
    "            if opt.is_main and step % opt.save_freq == 0:\n",
    "                src.util.save(model, optimizer, scheduler, step, best_dev_em,\n",
    "                          opt, checkpoint_path, f\"step-{step}\")\n",
    "            if step > opt.total_steps:\n",
    "                break\n",
    "\n",
    "def evaluate(model, dataset, tokenizer, collator, opt, step):\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=opt.per_gpu_batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=10,\n",
    "        collate_fn=collator\n",
    "    )\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    exactmatch = []\n",
    "    preds =[]\n",
    "    target =[]\n",
    "    model = model.module if hasattr(model, \"module\") else model\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            (idx, _, _, context_ids, context_mask) = batch\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                max_length=50\n",
    "            )\n",
    "\n",
    "            for k, o in enumerate(outputs):\n",
    "                ans = tokenizer.decode(o, skip_special_tokens=True)\n",
    "                gold = dataset.get_example(idx[k])['answers']\n",
    "                preds.append([ans]) \n",
    "                target.append(gold)\n",
    "                    # log=ans\n",
    "                    # log+=f\"-------\"\n",
    "                    # log+=gold[0]\n",
    "                    # log+=f\"=======\"\n",
    "                    # logger.info(log)\n",
    "                # score = src.evaluation.ems(ans, gold)\n",
    "                # total += 1\n",
    "                # exactmatch.append(score)\n",
    "        with open('answer2/golden_'+str(step)+'.txt','w') as f:\n",
    "            for i,line in enumerate(target):\n",
    "                f.write(str(i)+'\\t'+line[0]+'\\n')\n",
    "        with open('answer2/preds_'+str(step)+'.txt','w') as f:\n",
    "            for i,line in enumerate(preds):\n",
    "                f.write(str(i)+'\\t'+line[0]+'\\n')\n",
    "\n",
    "    exactmatch, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n",
    "    return exactmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/shifu/.conda/envs/fid4/lib/python3.8/site-packages/ipykernel_launcher.py',\n",
       " '--ip=127.0.0.1',\n",
       " '--stdin=9095',\n",
       " '--control=9093',\n",
       " '--hb=9092',\n",
       " '--Session.signature_scheme=\"hmac-sha256\"',\n",
       " '--Session.key=b\"4f8710b2-821a-4336-85c8-cd1f0c27bde2\"',\n",
       " '--shell=9094',\n",
       " '--transport=\"tcp\"',\n",
       " '--iopub=9096',\n",
       " '--f=/home/shifu/.local/share/jupyter/runtime/kernel-v2-3352m4eiONCdXpYg.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv=['/home/shifu/.conda/envs/fid2/lib/python3.8/site-packages/ipykernel_launcher.py'\n",
    "    ,'--train_data'\n",
    "    ,'/home/shifu/FiD/codefid_data3/train_php_10.json'\n",
    "    ,'--eval_data'\n",
    "    ,'/home/shifu/FiD/codefid_data3/test_php_10.json'\n",
    "    ,'--model_path'\n",
    "    ,'/home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000'\n",
    "    ,'--text_maxlength'\n",
    "    ,'200'\n",
    "    ,'--answer_maxlength'\n",
    "    ,'100'\n",
    "    ,'--save_freq'\n",
    "    ,'1500'\n",
    "    ,'--model_size'\n",
    "    ,'base'\n",
    "    ,'--per_gpu_batch_size'\n",
    "    ,'10'\n",
    "    ,'--total_steps'\n",
    "    ,'1000000'\n",
    "    ,'--n_context'\n",
    "    ,'10'\n",
    "    ,'--name'\n",
    "    ,'my_code_fid5'\n",
    "    ,'--checkpoint_dir'\n",
    "    ,'checkpoint'\n",
    "    ,'--use_checkpoint'\n",
    "    ,'--eval_freq'\n",
    "    ,'1500']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_reader_options()\n",
    "options.add_optim_options()\n",
    "opt = options.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt.seed)\n",
    "src.slurm.init_distributed_mode(opt)\n",
    "src.slurm.init_signal_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(opt.checkpoint_dir)/opt.name\n",
    "checkpoint_exists = checkpoint_path.exists()\n",
    "if opt.is_distributed:\n",
    "    torch.distributed.barrier()\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = src.util.init_logger(\n",
    "    opt.is_main,\n",
    "    opt.is_distributed,\n",
    "    checkpoint_path / 'run.log'\n",
    ")\n",
    "\n",
    "model_name = 't5-' + opt.model_size\n",
    "model_class = src.model.FiDT5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)\n",
    "collator = src.data.Collator(opt.text_maxlength, tokenizer, answer_maxlength=opt.answer_maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/25/2022 02:24:12] {util.py:75} INFO - Loading /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000\n",
      "[06/25/2022 02:24:12] {configuration_utils.py:262} INFO - loading configuration file /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000/config.json\n",
      "[06/25/2022 02:24:12] {configuration_utils.py:300} INFO - Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"FiDT5\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[06/25/2022 02:24:12] {modeling_utils.py:665} INFO - loading weights file /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000/pytorch_model.bin\n",
      "[06/25/2022 02:24:15] {modeling_utils.py:765} INFO - All model checkpoint weights were used when initializing FiDT5.\n",
      "\n",
      "[06/25/2022 02:24:15] {modeling_utils.py:773} INFO - All the weights of FiDT5 were initialized from the model checkpoint at /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use FiDT5 for predictions without further training.\n",
      "[06/25/2022 02:24:17] {util.py:78} INFO - loading checkpoint /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000/optimizer.pth.tar\n",
      "[06/25/2022 02:24:18] {3491006784.py:31} INFO - Model loaded from /home/shifu/FiD/checkpoint/my_code_fid4/checkpoint/step-3000\n"
     ]
    }
   ],
   "source": [
    "# use golbal rank and world size to split the eval set on multiple gpus\n",
    "train_examples = src.data.load_data(\n",
    "    opt.train_data, \n",
    "    global_rank=opt.global_rank, \n",
    "    world_size=opt.world_size,\n",
    ")\n",
    "train_dataset = src.data.Dataset(train_examples, opt.n_context)\n",
    "# use golbal rank and world size to split the eval set on multiple gpus\n",
    "eval_examples = src.data.load_data(\n",
    "    opt.eval_data,\n",
    "    global_rank=opt.global_rank,\n",
    "    world_size=opt.world_size,\n",
    ")\n",
    "eval_dataset = src.data.Dataset(eval_examples, opt.n_context)\n",
    "\n",
    "if not checkpoint_exists and opt.model_path == \"none\":\n",
    "    t5 = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    model = src.model.FiDT5(t5.config)\n",
    "    model.load_t5(t5.state_dict())\n",
    "    model = model.to(opt.local_rank)\n",
    "    optimizer, scheduler = src.util.set_optim(opt, model)\n",
    "    step, best_dev_em = 0, 0.0\n",
    "elif opt.model_path == \"none\":\n",
    "    load_path = checkpoint_path / 'checkpoint' / 'latest'\n",
    "    model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n",
    "        src.util.load(model_class, load_path, opt, reset_params=False)\n",
    "    logger.info(f\"Model loaded from {load_path}\")\n",
    "else:\n",
    "    model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n",
    "        src.util.load(model_class, opt.model_path, opt, reset_params=True)\n",
    "    logger.info(f\"Model loaded from {opt.model_path}\")\n",
    "\n",
    "model.set_checkpoint(opt.use_checkpoint)\n",
    "\n",
    "if opt.is_distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=[opt.local_rank],\n",
    "        output_device=opt.local_rank,\n",
    "        find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "# logger.info(\"Start training\")\n",
    "# train(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     scheduler,\n",
    "#     step,\n",
    "#     train_dataset,\n",
    "#     eval_dataset,\n",
    "#     opt,\n",
    "#     collator,\n",
    "#     best_dev_em,\n",
    "#     checkpoint_path\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     scheduler,\n",
    "#     step,\n",
    "#     train_dataset,\n",
    "#     eval_dataset,\n",
    "#     opt,\n",
    "#     collator,\n",
    "#     best_dev_em,\n",
    "#     checkpoint_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_bleu(model, dataset, tokenizer, collator, opt):\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=opt.per_gpu_batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=10,\n",
    "        collate_fn=collator\n",
    "    )\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    exactmatch = []\n",
    "    preds =[]\n",
    "    target =[]\n",
    "    model = model.module if hasattr(model, \"module\") else model\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # if i<1000:\n",
    "            #     continue\n",
    "            \n",
    "            (idx, _, _, context_ids, context_mask) = batch\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                max_length=100\n",
    "            )\n",
    "\n",
    "            for k, o in enumerate(outputs):\n",
    "                ans = tokenizer.decode(o, skip_special_tokens=True)\n",
    "                gold = dataset.get_example(idx[k])['answers']\n",
    "                score = src.evaluation.ems(ans, gold)\n",
    "                total += 1\n",
    "                exactmatch.append(score)\n",
    "                # print(\"=================\")\n",
    "                # print(ans)\n",
    "                # print(\"-----------------\")\n",
    "                # print(gold)\n",
    "            \n",
    "                preds.append([ans]) \n",
    "                target.append(gold)\n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "            if i>100:\n",
    "                break\n",
    "    print(i)\n",
    "    #exactmatch, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n",
    "    return preds,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler(eval_dataset)\n",
    "dataloader = DataLoader(eval_dataset,\n",
    "    sampler=sampler,\n",
    "    batch_size=opt.per_gpu_batch_size,\n",
    "    drop_last=False,\n",
    "    num_workers=10,\n",
    "    collate_fn=collator\n",
    ")\n",
    "for i, batch in enumerate(dataloader):\n",
    "    b= batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shifu/FiD/test_fid_train_reader.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m preds,target \u001b[39m=\u001b[39m Test_bleu(model, eval_dataset, tokenizer, collator, opt)\n",
      "\u001b[1;32m/home/shifu/FiD/test_fid_train_reader.ipynb Cell 13'\u001b[0m in \u001b[0;36mTest_bleu\u001b[0;34m(model, dataset, tokenizer, collator, opt)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=17'>18</a>\u001b[0m     \u001b[39m# if i<1000:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=18'>19</a>\u001b[0m     \u001b[39m#     continue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=19'>20</a>\u001b[0m     (idx, _, _, context_ids, context_mask) \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=21'>22</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=22'>23</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49mcontext_ids\u001b[39m.\u001b[39;49mcuda(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=23'>24</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcontext_mask\u001b[39m.\u001b[39;49mcuda(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=24'>25</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=25'>26</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(outputs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.147/home/shifu/FiD/test_fid_train_reader.ipynb#ch0000011vscode-remote?line=28'>29</a>\u001b[0m         ans \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(o, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/FiD/src/model.py:51\u001b[0m, in \u001b[0;36mFiDT5.generate\u001b[0;34m(self, input_ids, attention_mask, max_length)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, max_length):\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mn_passages \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     52\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mview(input_ids\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     53\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask\u001b[39m.\u001b[39;49mview(attention_mask\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     54\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length\n\u001b[1;32m     55\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/generation_utils.py:462\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_beam_search(\n\u001b[1;32m    437\u001b[0m         input_ids,\n\u001b[1;32m    438\u001b[0m         cur_len\u001b[39m=\u001b[39mcur_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m         model_specific_kwargs\u001b[39m=\u001b[39mmodel_specific_kwargs,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_no_beam_search(\n\u001b[1;32m    463\u001b[0m         input_ids,\n\u001b[1;32m    464\u001b[0m         cur_len\u001b[39m=\u001b[39;49mcur_len,\n\u001b[1;32m    465\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    466\u001b[0m         min_length\u001b[39m=\u001b[39;49mmin_length,\n\u001b[1;32m    467\u001b[0m         do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m    468\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    469\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    470\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    471\u001b[0m         repetition_penalty\u001b[39m=\u001b[39;49mrepetition_penalty,\n\u001b[1;32m    472\u001b[0m         no_repeat_ngram_size\u001b[39m=\u001b[39;49mno_repeat_ngram_size,\n\u001b[1;32m    473\u001b[0m         bad_words_ids\u001b[39m=\u001b[39;49mbad_words_ids,\n\u001b[1;32m    474\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m    475\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m    476\u001b[0m         batch_size\u001b[39m=\u001b[39;49meffective_batch_size,\n\u001b[1;32m    477\u001b[0m         encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m    478\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    479\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    480\u001b[0m         model_specific_kwargs\u001b[39m=\u001b[39;49mmodel_specific_kwargs,\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    483\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/generation_utils.py:520\u001b[0m, in \u001b[0;36mGenerationMixin._generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mwhile\u001b[39;00m cur_len \u001b[39m<\u001b[39m max_length:\n\u001b[1;32m    516\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(\n\u001b[1;32m    517\u001b[0m         input_ids, past\u001b[39m=\u001b[39mpast, attention_mask\u001b[39m=\u001b[39mattention_mask, use_cache\u001b[39m=\u001b[39muse_cache, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_specific_kwargs\n\u001b[1;32m    518\u001b[0m     )\n\u001b[0;32m--> 520\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    521\u001b[0m     next_token_logits \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m][:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    523\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess_next_token_scores(\n\u001b[1;32m    524\u001b[0m         scores\u001b[39m=\u001b[39mnext_token_logits,\n\u001b[1;32m    525\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m         num_beams\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/FiD/src/model.py:42\u001b[0m, in \u001b[0;36mFiDT5.forward\u001b[0;34m(self, input_ids, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mview(attention_mask\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m     43\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     44\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     45\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:1126\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_past_key_value_states, use_cache, labels, inputs_embeds, decoder_inputs_embeds, head_mask, output_attentions, output_hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         decoder_inputs_embeds \u001b[39m=\u001b[39m decoder_inputs_embeds[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m   1125\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1126\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1127\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1128\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1129\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1130\u001b[0m     past_key_value_states\u001b[39m=\u001b[39;49mdecoder_past_key_value_states,\n\u001b[1;32m   1131\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1132\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1133\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1134\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1135\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1136\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1137\u001b[0m )\n\u001b[1;32m   1139\u001b[0m \u001b[39m# insert decoder past at right place\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[39m# to speed up decoding\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:738\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    736\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 738\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    739\u001b[0m     hidden_states,\n\u001b[1;32m    740\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    741\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    742\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    743\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    744\u001b[0m     encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    745\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    746\u001b[0m     past_key_value_state\u001b[39m=\u001b[39;49mpast_key_value_state,\n\u001b[1;32m    747\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    748\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    749\u001b[0m )\n\u001b[1;32m    750\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[1;32m    752\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m layer_outputs[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:529\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value_state, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[1;32m    530\u001b[0m     hidden_states,\n\u001b[1;32m    531\u001b[0m     kv\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    532\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    533\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    534\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    535\u001b[0m     past_key_value_state\u001b[39m=\u001b[39;49mcross_attn_past_key_value_state,\n\u001b[1;32m    536\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    537\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    538\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    539\u001b[0m )\n\u001b[1;32m    540\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    541\u001b[0m \u001b[39m# Combine self attn and cross attn key value states\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:451\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, kv, attention_mask, position_bias, head_mask, past_key_value_state, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    440\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    449\u001b[0m ):\n\u001b[1;32m    450\u001b[0m     norm_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 451\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[1;32m    452\u001b[0m         norm_x,\n\u001b[1;32m    453\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    454\u001b[0m         kv\u001b[39m=\u001b[39;49mkv,\n\u001b[1;32m    455\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    456\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    457\u001b[0m         past_key_value_state\u001b[39m=\u001b[39;49mpast_key_value_state,\n\u001b[1;32m    458\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    459\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    460\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m     y \u001b[39m=\u001b[39m attention_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m    463\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(y)\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:366\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, input, mask, kv, position_bias, past_key_value_state, head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_relative_attention_bias:\n\u001b[1;32m    365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo position_bias provided and no weights to compute position_bias\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m position_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_bias(real_qlen, klen)\n\u001b[1;32m    368\u001b[0m \u001b[39m# if key and values are already calculated\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m# we want only the last query position bias\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m past_key_value_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fid4/lib/python3.8/site-packages/transformers/modeling_t5.py:283\u001b[0m, in \u001b[0;36mT5Attention.compute_bias\u001b[0;34m(self, qlen, klen)\u001b[0m\n\u001b[1;32m    281\u001b[0m context_position \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(qlen, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    282\u001b[0m memory_position \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(klen, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)[\u001b[39mNone\u001b[39;00m, :]\n\u001b[0;32m--> 283\u001b[0m relative_position \u001b[39m=\u001b[39m memory_position \u001b[39m-\u001b[39;49m context_position  \u001b[39m# shape (qlen, klen)\u001b[39;00m\n\u001b[1;32m    284\u001b[0m rp_bucket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_relative_position_bucket(\n\u001b[1;32m    285\u001b[0m     relative_position,  \u001b[39m# shape (qlen, klen)\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     bidirectional\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder,\n\u001b[1;32m    287\u001b[0m     num_buckets\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention_num_buckets,\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m rp_bucket \u001b[39m=\u001b[39m rp_bucket\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention_bias\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds,target = Test_bleu(model, eval_dataset, tokenizer, collator, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = SequentialSampler(train_dataset)\n",
    "# dataloader = DataLoader(train_dataset,\n",
    "#         sampler=sampler,\n",
    "#         batch_size=opt.per_gpu_batch_size,\n",
    "#         drop_last=False,\n",
    "#         num_workers=10,\n",
    "#         collate_fn=collator\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataloader):\n",
    "    \n",
    "#     (idx, _, _, context_ids, context_mask) = batch\n",
    "\n",
    "\n",
    "#     gold = train_dataset.get_example(idx[0])['question']\n",
    "#     print(gold)\n",
    "#     if i >10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('golden.txt','w') as f:\n",
    "    for i,line in enumerate(target):\n",
    "        f.write(str(i)+'\\t'+line[0]+'\\n')\n",
    "with open('preds.txt','w') as f:\n",
    "    for i,line in enumerate(preds):\n",
    "        f.write(str(i)+'\\t'+line[0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "results = metric.compute(predictions=preds, references=target)\n",
    "print(results[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "ans,gold =Test_bleu(model, eval_dataset, tokenizer, collator, opt)\n",
    "results = metric.compute(predictions=ans, references=gold)\n",
    "print(results[\"score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fid4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "584a83d5a14c4008d93e5e43cb346bef978962d6043aff91bd151d2b1bb783ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
